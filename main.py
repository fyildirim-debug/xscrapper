import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from urllib.parse import unquote, urlencode
from fastapi import FastAPI, Query, Path
from typing import Optional, List
from datetime import date, datetime
import uvicorn
from pydantic import BaseModel, Field
from enum import Enum
import os
import json

DOMAIN = "https://nitter.net"
TWITTER_IMG_DOMAIN = "https://pbs.twimg.com"

class FilterType(str, Enum):
    nativeretweets = "nativeretweets"
    media = "media"
    videos = "videos"
    news = "news"
    verified = "verified"
    native_video = "native_video"
    replies = "replies"
    links = "links"
    images = "images"
    safe = "safe"
    quote = "quote"
    pro_video = "pro_video"

app = FastAPI(
    title="Twitter Arama API",
    description="""
    Nitter Ã¼zerinden Twitter aramasÄ± yapmanÄ±zÄ± saÄŸlayan API.
    
    ## Ã–zellikler
    * Tweet aramasÄ± yapma
    * Ã‡eÅŸitli filtreler kullanma
    * Tarih aralÄ±ÄŸÄ± belirleme
    * Konum bazlÄ± arama
    
    ## Ã–rnek KullanÄ±mlar
    * Basit arama: `/api/search?q=python`
    * Filtreli arama: `/api/search?q=python&include_filters=images,verified`
    * Tarih aralÄ±klÄ± arama: `/api/search?q=python&since=2024-01-01&until=2024-03-20`
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

@app.get("/", include_in_schema=False)
async def root():
    return {"message": "Server is ok"}

@app.get("/api/user/{username}",
         summary="Twitter KullanÄ±cÄ± Profili",
         response_description="KullanÄ±cÄ± profili ve tweet bilgileri")
async def get_user_profile(
    username: str = Path(..., 
                      description="Twitter kullanÄ±cÄ± adÄ± (@iÅŸareti olmadan)",
                      examples=["elonmusk", "BillGates"],
                      min_length=1),
    max_tweets: int = Query(0,
                          description="Getirilecek maksimum tweet sayÄ±sÄ± (0: sadece profil bilgileri)",
                          ge=0,
                          le=1000)
):
    """
    Twitter kullanÄ±cÄ±sÄ±nÄ±n profil bilgilerini ve tweetlerini getirir.
    
    ## Parametreler
    * **username**: Twitter kullanÄ±cÄ± adÄ± (@iÅŸareti olmadan) (zorunlu)
    * **max_tweets**: Getirilecek maksimum tweet sayÄ±sÄ± (varsayÄ±lan: 0, sadece profil bilgileri)
    
    ## DÃ¶nÃ¼ÅŸ DeÄŸerleri
    * **profile**: KullanÄ±cÄ± profil bilgileri
        * twitter_url: GerÃ§ek Twitter profil URL'i
        * display_name: GÃ¶rÃ¼nen adÄ±
        * username: KullanÄ±cÄ± adÄ± (@ile)
        * stats:
            * tweets: Tweet sayÄ±sÄ±
            * following: Takip edilen sayÄ±sÄ±
            * followers: TakipÃ§i sayÄ±sÄ±
            * likes: BeÄŸeni sayÄ±sÄ±
            * media: Medya sayÄ±sÄ±
    * **tweets**: KullanÄ±cÄ±nÄ±n tweetleri (max_tweets > 0 ise)
        * content: Tweet iÃ§eriÄŸi
        * date: Tweet tarihi
        * stats:
            * likes: BeÄŸeni sayÄ±sÄ±
            * comments: Yorum sayÄ±sÄ±
            * retweets: Retweet sayÄ±sÄ±
        * media: Tweet'teki medya URL'leri
        * tweet_link: Tweet'in linki
    
    ## Ã–rnek Ä°stekler
    ```
    /api/user/elonmusk          # Sadece profil bilgileri
    /api/user/BillGates?max_tweets=100  # Profil bilgileri ve 100 tweet
    ```
    """
    scraper = TwitterScrapper()
    
    try:
        results = await scraper.get_profile(username, max_tweets)
        if not results or not results.get("profile_data"):
            return {
                "error": "KullanÄ±cÄ± profili bulunamadÄ±",
                "message": "Profil bilgileri alÄ±namadÄ± veya kullanÄ±cÄ± mevcut deÄŸil"
            }
            
        return results["profile_data"]
        
    except Exception as e:
        return {
            "error": "KullanÄ±cÄ± profili alÄ±namadÄ±",
            "message": str(e)
        }

class HTMLCache:
    def __init__(self, cache_dir="cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_filename(self, url: str) -> str:
        """URL'yi dosya adÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
        safe_filename = "".join(c if c.isalnum() else "_" for c in url)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return os.path.join(self.cache_dir, f"{safe_filename}_{timestamp}.html")
    
    def save_html(self, url: str, html_content: str) -> str:
        """HTML iÃ§eriÄŸini kaydeder ve dosya yolunu dÃ¶ndÃ¼rÃ¼r."""
        filename = self._get_cache_filename(url)
        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_content)
        return filename
    
    def get_html(self, filename: str) -> Optional[str]:
        """KaydedilmiÅŸ HTML iÃ§eriÄŸini okur."""
        try:
            with open(filename, "r", encoding="utf-8") as f:
                return f.read()
        except:
            return None

class SearchMetadata:
    def __init__(self, cache_dir="cache"):
        self.cache_dir = cache_dir
        self.metadata_file = os.path.join(cache_dir, "search_metadata.json")
        os.makedirs(cache_dir, exist_ok=True)
        self.load_metadata()
    
    def load_metadata(self):
        """Metadata dosyasÄ±nÄ± yÃ¼kler."""
        try:
            with open(self.metadata_file, "r", encoding="utf-8") as f:
                self.metadata = json.load(f)
        except:
            self.metadata = []
    
    def save_metadata(self):
        """Metadata dosyasÄ±nÄ± kaydeder."""
        with open(self.metadata_file, "w", encoding="utf-8") as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
    
    def add_search(self, query: str, url: str, html_file: str, params: dict):
        """Yeni arama bilgisini ekler."""
        search_data = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "url": url,
            "html_file": html_file,
            "parameters": params
        }
        self.metadata.append(search_data)
        self.save_metadata()

class TwitterScrapper:
    """
     --------- Twitter Scrapper ---------
     @author Blessing Ajala | Software Engineer ğŸ‘¨â€ğŸ’»
     @github https://github.com/Oyelamin

    """
    def __init__(self):
        """Initialize the Selenium WebDriver and cache system."""
        self.chrome_options = Options()
        self.chrome_options.add_argument("--headless=new")
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--window-size=1920x1080")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--log-level=3")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        self.chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        self.chrome_options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)

        self.service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(options=self.chrome_options)
        self.driver.implicitly_wait(10)  # 10 saniye bekleme sÃ¼resi
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # Cache sistemini baÅŸlat
        self.html_cache = HTMLCache()
        self.search_metadata = SearchMetadata()

    def __del__(self):
        """Ensure the WebDriver is properly closed when the object is deleted."""
        self.driver.quit()
        self.executor.shutdown(wait=True)

    @staticmethod
    def username_cleaner(username: str) -> str:
        return username.replace("@", "")

    @staticmethod
    def stat_cleaner(stat: str) -> int:
        """SayÄ±sal deÄŸerleri temizler ve int'e Ã§evirir.
        Ã–rnek: "1,234" -> 1234
               "12.3K" -> 12300
               "1.5M" -> 1500000
               "2.3B" -> 2300000000
        """
        # None veya boÅŸ string kontrolÃ¼
        if stat is None or not isinstance(stat, str):
            return 0
            
        # BoÅŸ string veya sadece boÅŸluk karakteri kontrolÃ¼
        stat = stat.strip()
        if not stat:
            return 0
            
        try:
            # VirgÃ¼lleri kaldÄ±r
            stat = stat.replace(",", "")
            
            # K/M/B formatÄ±nÄ± kontrol et
            if "K" in stat.upper():
                num = float(stat.upper().replace("K", ""))
                return int(num * 1000)
            elif "M" in stat.upper():
                num = float(stat.upper().replace("M", ""))
                return int(num * 1000000)
            elif "B" in stat.upper():
                num = float(stat.upper().replace("B", ""))
                return int(num * 1000000000)
            
            # SayÄ±sal deÄŸer kontrolÃ¼
            if not any(c.isdigit() for c in stat):
                return 0
                
            return int(float(stat))
        except:
            return 0

    @staticmethod
    def convert_nitter_image_to_twitter(nitter_url: str) -> str:
        """Converts Nitter image URLs to the corresponding Twitter image URLs."""
        if nitter_url.startswith(f"{DOMAIN}/pic/"):
            return unquote(nitter_url.replace(f"{DOMAIN}/pic/", f"{TWITTER_IMG_DOMAIN}/"))
        return nitter_url

    def build_search_url(self, 
                        query: str,
                        include_filters: List[str] = None,
                        exclude_filters: List[str] = None,
                        since: str = None,
                        until: str = None) -> str:
        """Arama URL'sini oluÅŸturur."""
        params = {
            "f": "tweets",
            "q": query
        }
        
        # Dahil edilecek filtreleri ekle
        if include_filters:
            for filter_name in include_filters:
                params[f"f-{filter_name}"] = "on"
        
        # HariÃ§ tutulacak filtreleri ekle
        if exclude_filters:
            for filter_name in exclude_filters:
                params[f"e-{filter_name}"] = "on"
        
        # Tarih aralÄ±ÄŸÄ±
        if since:
            params["since"] = since
        if until:
            params["until"] = until
        
        return f"{DOMAIN}/search?{urlencode(params)}"

    def search_html_contents(self, 
                           query: str,
                           include_filters: List[str] = None,
                           exclude_filters: List[str] = None,
                           since: str = None,
                           until: str = None,
                           max_tweets: int = 50) -> str | None:
        """Arama sayfasÄ±nÄ±n HTML iÃ§eriÄŸini getirir ve kaydeder."""
        base_url = self.build_search_url(
            query=query,
            include_filters=include_filters,
            exclude_filters=exclude_filters,
            since=since,
            until=until
        )

        try:
            self.driver.get(base_url)
            time.sleep(10)
            
            # Ä°lk tweet sayÄ±sÄ±nÄ± al
            initial_tweet_count = len(self.driver.find_elements("css selector", ".timeline-item"))
            print(f"BaÅŸlangÄ±Ã§ tweet sayÄ±sÄ±: {initial_tweet_count}")
            
            all_tweets = []  # TÃ¼m tweetleri tutacak liste
            current_html = self.driver.page_source
            
            # Ä°lk sayfadaki tweetleri ekle
            soup = BeautifulSoup(current_html, 'html.parser')
            tweets = soup.select(".timeline-item")
            for tweet in tweets:
                if not "show-more" in tweet.get("class", []):
                    all_tweets.append(str(tweet))
            
            total_tweets = len(all_tweets)
            page_count = 1
            
            # Ä°stenen tweet sayÄ±sÄ±na ulaÅŸana kadar devam et
            while total_tweets < max_tweets:
                try:
                    # Load more butonunu ve URL'sini bul
                    show_more_links = soup.select(".show-more a")
                    
                    # "Load more" iÃ§eren linki bul (Load newest deÄŸil)
                    load_more = None
                    for link in show_more_links:
                        if "Load newest" not in link.text and "cursor=" in link.get('href', ''):
                            load_more = link
                            break
                    
                    if not load_more:
                        print("Load more linki bulunamadÄ±, mevcut tweetlerle devam ediliyor.")
                        break
                        
                    # Yeni URL'yi oluÅŸtur
                    next_page_url = f"{DOMAIN}/search{load_more['href']}"
                    print(f"Sayfa {page_count + 1} yÃ¼kleniyor: {next_page_url}")
                    
                    # Yeni sayfayÄ± yÃ¼kle
                    self.driver.get(next_page_url)
                    time.sleep(3)
                    
                    # Yeni HTML'i al
                    current_html = self.driver.page_source
                    soup = BeautifulSoup(current_html, 'html.parser')
                    
                    # Yeni sayfadaki tweetleri ekle
                    previous_count = total_tweets
                    new_tweets = soup.select(".timeline-item")
                    for tweet in new_tweets:
                        if not "show-more" in tweet.get("class", []):
                            all_tweets.append(str(tweet))
                    
                    # Toplam tweet sayÄ±sÄ±nÄ± gÃ¼ncelle
                    total_tweets = len(all_tweets)
                    print(f"Sayfa {page_count + 1} sonrasÄ± toplam tweet sayÄ±sÄ±: {total_tweets}")
                    
                    # EÄŸer yeni tweet eklenemediyse dur
                    if total_tweets <= previous_count:
                        print("Yeni tweet eklenemedi, mevcut tweetlerle devam ediliyor.")
                        break
                    
                    page_count += 1
                    
                except Exception as e:
                    print(f"Load more error: {e}")
                    break
            
            # Son tweet sayÄ±sÄ±nÄ± gÃ¶ster
            print(f"Toplam tweet sayÄ±sÄ±: {total_tweets}")
            
            # TÃ¼m tweetleri tek bir HTML iÃ§inde birleÅŸtir
            timeline_html = f"""
            <div class="timeline">
                {"".join(all_tweets)}
            </div>
            """
            
            # HTML iÃ§eriÄŸini kaydet
            html_file = self.html_cache.save_html(base_url, timeline_html)
            
            # Metadata'yÄ± kaydet
            self.search_metadata.add_search(
                query=query,
                url=base_url,
                html_file=html_file,
                params={
                    "include_filters": include_filters,
                    "exclude_filters": exclude_filters,
                    "since": since,
                    "until": until,
                    "total_tweets": total_tweets,
                    "requested_tweets": max_tweets,
                    "pages_loaded": page_count
                }
            )
            
            return timeline_html
        except Exception as e:
            print("Error:", e)
            return None

    async def search(self, 
                    query: str,
                    include_filters: List[str] = None,
                    exclude_filters: List[str] = None,
                    since: str = None,
                    until: str = None,
                    max_tweets: int = 50) -> object:
        """Search for tweets based on query and filters (async)."""
        html = await self.run_in_thread(
            self.search_html_contents,
            query=query,
            include_filters=include_filters,
            exclude_filters=exclude_filters,
            since=since,
            until=until,
            max_tweets=max_tweets
        )
        return await self.extract_search_contents(html)

    async def get_profile(self, username: str, max_tweets: int = 50) -> object:
        """Get profile information of a user (async)."""
        html, stats = await self.run_in_thread(self.profile_html_contents, username, max_tweets)
        profile_data = await self.extract_profile_contents(html, max_tweets) if html else None
        return {
            "stats": stats,
            "profile_data": profile_data
        }

    async def run_in_thread(self, func, *args, **kwargs):
        """Run blocking functions inside a ThreadPoolExecutor asynchronously."""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(
            self.executor,
            lambda: func(*args, **kwargs)
        )

    def profile_html_contents(self, username: str, max_tweets: int = 50) -> tuple[str | None, dict]:
        """KullanÄ±cÄ± profil sayfasÄ±nÄ±n HTML iÃ§eriÄŸini getirir."""
        url = f"{DOMAIN}/{self.username_cleaner(username)}"
        stats = {
            "total_tweets": 0,
            "pages_loaded": 1,
            "requested_tweets": max_tweets
        }

        try:
            self.driver.get(url)
            time.sleep(10)  # SayfanÄ±n yÃ¼klenmesi iÃ§in bekle
            
            # Profil bilgilerinin yÃ¼klendiÄŸinden emin ol
            profile_card = self.driver.find_element("css selector", ".profile-card")
            if not profile_card:
                raise Exception("Profil bilgileri yÃ¼klenemedi")
            
            # Ä°lk tweet sayÄ±sÄ±nÄ± al
            initial_tweet_count = len(self.driver.find_elements("css selector", ".timeline-item"))
            print(f"BaÅŸlangÄ±Ã§ tweet sayÄ±sÄ±: {initial_tweet_count}")
            
            all_tweets = []  # TÃ¼m tweetleri tutacak liste
            current_html = self.driver.page_source
            
            # Ä°lk sayfadaki tweetleri ekle
            soup = BeautifulSoup(current_html, 'html.parser')
            tweets = soup.select(".timeline-item")
            for tweet in tweets:
                if not "show-more" in tweet.get("class", []):
                    all_tweets.append(str(tweet))
            
            total_tweets = len(all_tweets)
            page_count = 1
            
            # Ä°stenen tweet sayÄ±sÄ±na ulaÅŸana kadar devam et
            while total_tweets < max_tweets:
                try:
                    # Load more butonunu ve URL'sini bul
                    show_more_links = soup.select(".show-more a")
                    
                    # "Load more" iÃ§eren linki bul (Load newest deÄŸil)
                    load_more = None
                    for link in show_more_links:
                        if "Load newest" not in link.text and "cursor=" in link.get('href', ''):
                            load_more = link
                            break
                    
                    if not load_more:
                        print("Load more linki bulunamadÄ±, mevcut tweetlerle devam ediliyor.")
                        break
                        
                    # Yeni URL'yi oluÅŸtur
                    next_page_url = f"{DOMAIN}{load_more['href']}"
                    print(f"Sayfa {page_count + 1} yÃ¼kleniyor: {next_page_url}")
                    
                    # Yeni sayfayÄ± yÃ¼kle
                    self.driver.get(next_page_url)
                    time.sleep(3)
                    
                    # Yeni HTML'i al
                    current_html = self.driver.page_source
                    soup = BeautifulSoup(current_html, 'html.parser')
                    
                    # Yeni sayfadaki tweetleri ekle
                    previous_count = total_tweets
                    new_tweets = soup.select(".timeline-item")
                    for tweet in new_tweets:
                        if not "show-more" in tweet.get("class", []):
                            all_tweets.append(str(tweet))
                    
                    # Toplam tweet sayÄ±sÄ±nÄ± gÃ¼ncelle
                    total_tweets = len(all_tweets)
                    print(f"Sayfa {page_count + 1} sonrasÄ± toplam tweet sayÄ±sÄ±: {total_tweets}")
                    
                    # EÄŸer yeni tweet eklenemediyse dur
                    if total_tweets <= previous_count:
                        print("Yeni tweet eklenemedi, mevcut tweetlerle devam ediliyor.")
                        break
                    
                    page_count += 1
                    
                except Exception as e:
                    print(f"Load more error: {e}")
                    break
            
            # Son tweet sayÄ±sÄ±nÄ± gÃ¶ster
            print(f"Toplam tweet sayÄ±sÄ±: {total_tweets}")
            
            # TÃ¼m tweetleri tek bir HTML iÃ§inde birleÅŸtir
            timeline_html = f"""
            <div class="profile-card">{str(profile_card.get_attribute('outerHTML'))}</div>
            <div class="timeline">
                {"".join(all_tweets)}
            </div>
            """
            
            # HTML iÃ§eriÄŸini kaydet
            html_file = self.html_cache.save_html(url, timeline_html)
            
            # Metadata'yÄ± kaydet
            stats.update({
                "total_tweets": total_tweets,
                "pages_loaded": page_count
            })
            self.search_metadata.add_search(
                query=username,
                url=url,
                html_file=html_file,
                params=stats
            )
            
            return timeline_html, stats
            
        except Exception as e:
            print(f"Profil sayfasÄ± yÃ¼klenirken hata: {e}")
            return None, stats

    async def extract_profile_contents(self, html: str, max_tweets: int = 0) -> object:
        """Profil detaylarÄ±nÄ±, tweetleri ve medyayÄ± Ã§Ä±karÄ±r."""
        if not html:
            return None
            
        soup = BeautifulSoup(html, 'html.parser')
        
        # GÃ¼venli element seÃ§imi iÃ§in yardÄ±mcÄ± fonksiyon
        def safe_select(selector, attr="text", default=""):
            element = soup.select_one(selector)
            if not element:
                return default
            if attr == "text":
                return element.text.strip()
            return element.get(attr, default)

        # Profil bilgilerini gÃ¼venli ÅŸekilde al
        username = safe_select('.profile-card-username', 'text')
        twitter_url = f"https://x.com/{username.replace('@', '')}"
        display_name = safe_select('.profile-card-fullname', 'text')
        
        # Ä°statistikleri al
        tweets_count = self.stat_cleaner(safe_select('.posts .profile-stat-num', 'text'))
        following_count = self.stat_cleaner(safe_select('.following .profile-stat-num', 'text'))
        followers_count = self.stat_cleaner(safe_select('.followers .profile-stat-num', 'text'))
        likes_count = self.stat_cleaner(safe_select('.likes .profile-stat-num', 'text'))
        
        # Profil resmini al
        profile_image = ""
        avatar_link = soup.select_one('.profile-card-avatar')
        if avatar_link:
            img_src = avatar_link.get('href', '')
            if img_src:
                profile_image = self.convert_nitter_image_to_twitter(DOMAIN + img_src)
        
        # Medya sayÄ±sÄ±nÄ± al
        media_count = 0
        media_text = safe_select('.photo-rail-header a', 'text')
        if media_text:
            # "3,380 Photos and videos" formatÄ±ndan sayÄ±yÄ± Ã§Ä±kar
            try:
                # Metni temizle ve ilk sayÄ±yÄ± al
                media_count = self.stat_cleaner(''.join(c for c in media_text.split()[0] if c.isdigit() or c in ',.KMB'))
            except:
                media_count = 0
        
        profile_info = {
            "twitter_url": twitter_url,
            "display_name": display_name,
            "username": username,
            "profile_image": profile_image,
            "stats": {
                "tweets": tweets_count,
                "following": following_count,
                "followers": followers_count,
                "likes": likes_count,
                "media": media_count
            }
        }
        
        # max_tweets=0 ise sadece profil bilgilerini dÃ¶ndÃ¼r
        if max_tweets == 0:
            return {"profile": profile_info}

        tweets = []
        seen_tweet_ids = set()  # Tweet ID'lerini takip etmek iÃ§in set
        
        # Timeline'daki tÃ¼m tweetleri topla
        timeline_items = soup.select(".timeline-item")
        print(f"Bulunan tweet sayÄ±sÄ±: {len(timeline_items)}")
        
        for tweet in timeline_items:
            # Show more butonlarÄ±nÄ± atla
            if "show-more" in tweet.get("class", []):
                continue
                
            # Tweet linkinden ID'yi Ã§Ä±kar
            tweet_link = ""
            tweet_link_element = tweet.select_one(".tweet-link")
            if tweet_link_element and tweet_link_element.get("href"):
                tweet_link = tweet_link_element["href"]
                tweet_id = tweet_link.split('/status/')[1].split('#')[0] if '/status/' in tweet_link else ""
                
                # EÄŸer bu tweet ID'sini daha Ã¶nce gÃ¶rdÃ¼ysek, bu tweet'i atla
                if tweet_id in seen_tweet_ids:
                    continue
                    
                seen_tweet_ids.add(tweet_id)
                tweet_link = f"https://x.com/{username.replace('@', '')}/status/{tweet_id}"
            
            tweet_images = [
                self.convert_nitter_image_to_twitter(DOMAIN + img["src"]) 
                for img in tweet.select(".attachment.image img")
            ]
            
            # Tweet istatistiklerini gÃ¼venli ÅŸekilde al
            stats = {}
            for stat_type, icon_class in [("likes", "icon-heart"), ("comments", "icon-comment"), ("retweets", "icon-retweet")]:
                stat_element = tweet.select_one(f".{icon_class}")
                if stat_element and stat_element.parent:
                    stats[stat_type] = self.stat_cleaner(stat_element.parent.text.strip())
                else:
                    stats[stat_type] = 0
            
            # Tweet iÃ§eriÄŸini al
            content = ""
            content_element = tweet.select_one(".tweet-content")
            if content_element:
                content = content_element.text.strip()
            
            # Tweet tarihini al
            date = ""
            date_element = tweet.select_one(".tweet-date a")
            if date_element:
                date = date_element.text.strip()
            
            tweet_data = {
                "content": content,
                "date": date,
                "stats": stats,
                "media": tweet_images,
                "tweet_link": tweet_link
            }
            
            tweets.append(tweet_data)
            
            # Ä°stenen tweet sayÄ±sÄ±na ulaÅŸtÄ±ysak dÃ¶ngÃ¼yÃ¼ sonlandÄ±r
            if len(tweets) >= max_tweets:
                break

        print(f"DÃ¶ndÃ¼rÃ¼len tweet sayÄ±sÄ±: {len(tweets)}")
        
        return {
            "profile": profile_info,
            "tweets": tweets
        }

    async def extract_search_contents(self, html: str) -> list:
        """Extract tweet search results from the HTML content."""
        soup = BeautifulSoup(html, 'html.parser')
        tweets = []

        for tweet in soup.select(".timeline-item"):
            tweet_images = [self.convert_nitter_image_to_twitter(DOMAIN + img["src"]) 
                          for img in tweet.select(".attachment.image img")]
            
            username = tweet.select_one(".username")
            fullname = tweet.select_one(".fullname")
            tweet_link = tweet.select_one(".tweet-link")
            
            if username and fullname:
                # Tweet linkini X.com formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
                x_link = ""
                if tweet_link and tweet_link.get('href'):
                    # /username/status/123456789#m formatÄ±ndan ID'yi al
                    href = tweet_link['href']
                    if '/status/' in href:
                        tweet_id = href.split('/status/')[1].split('#')[0]
                        username_clean = username.text.strip().replace("@", "")
                        x_link = f"https://x.com/{username_clean}/status/{tweet_id}"
                
                tweet_data = {
                    "username": username.text.strip(),
                    "full_name": fullname.text.strip(),
                    "content": tweet.select_one(".tweet-content").text.strip() if tweet.select_one(".tweet-content") else "",
                    "date": tweet.select_one(".tweet-date a").text.strip() if tweet.select_one(".tweet-date a") else "",
                    "likes": tweet.select_one(".icon-heart").parent.text.strip() if tweet.select_one(".icon-heart") else "0",
                    "retweets": tweet.select_one(".icon-retweet").parent.text.strip() if tweet.select_one(".icon-retweet") else "0",
                    "comments": tweet.select_one(".icon-comment").parent.text.strip() if tweet.select_one(".icon-comment") else "0",
                    "images": tweet_images,
                    "profile_image": self.convert_nitter_image_to_twitter(DOMAIN + tweet.select_one(".tweet-avatar img")["src"]) if tweet.select_one(".tweet-avatar img") else "",
                    "tweet_link": x_link
                }
                tweets.append(tweet_data)

        return tweets

# FastAPI route'larÄ±
@app.get("/api/search", 
         summary="Twitter'da Tweet AramasÄ±",
         response_description="Arama sonuÃ§larÄ± ve kullanÄ±lan parametreler")
async def search_tweets(
    q: str = Query(..., 
                  description="Arama sorgusu", 
                  examples=["python programming", "galatasaray", "yapay zeka"],
                  min_length=1),
    include_filters: List[FilterType] = Query(None, 
                                            description="Dahil edilecek filtreler",
                                            examples=[["images", "verified"], ["media", "links"]]),
    exclude_filters: List[FilterType] = Query(None, 
                                            description="HariÃ§ tutulacak filtreler",
                                            examples=[["replies"], ["nativeretweets"]]),
    since: Optional[date] = Query(None, 
                                 description="Bu tarihten itibaren ara (YYYY-MM-DD)",
                                 examples=["2024-01-01"]),
    until: Optional[date] = Query(None, 
                                 description="Bu tarihe kadar ara (YYYY-MM-DD)",
                                 examples=["2024-03-20"]),
    max_tweets: int = Query(50, 
                          description="Maksimum tweet sayÄ±sÄ±",
                          ge=1,
                          le=1000)
):
    """
    Twitter'da tweet aramasÄ± yapar.
    
    ## Parametreler
    * **q**: Arama sorgusu (zorunlu)
    * **include_filters**: Dahil edilecek filtreler (isteÄŸe baÄŸlÄ±)
    * **exclude_filters**: HariÃ§ tutulacak filtreler (isteÄŸe baÄŸlÄ±)
    * **since**: BaÅŸlangÄ±Ã§ tarihi (isteÄŸe baÄŸlÄ±)
    * **until**: BitiÅŸ tarihi (isteÄŸe baÄŸlÄ±)
    * **max_tweets**: Maksimum tweet sayÄ±sÄ± (varsayÄ±lan: 50, min: 1, max: 1000)
    
    ## Filtreler
    * **nativeretweets**: Retweetler
    * **media**: Medya iÃ§eren tweetler
    * **videos**: Video iÃ§eren tweetler
    * **news**: Haber iÃ§eren tweetler
    * **verified**: OnaylÄ± hesaplarÄ±n tweetleri
    * **native_video**: Native video iÃ§eren tweetler
    * **replies**: YanÄ±tlar
    * **links**: Link iÃ§eren tweetler
    * **images**: Resim iÃ§eren tweetler
    * **safe**: GÃ¼venli iÃ§erik
    * **quote**: AlÄ±ntÄ± tweetler
    * **pro_video**: Pro video iÃ§eren tweetler
    
    ## Ã–rnek Ä°stekler
    ```
    /api/search?q=python
    /api/search?q=python&include_filters=images,verified
    /api/search?q=python&since=2024-01-01&until=2024-03-20
    /api/search?q=python&max_tweets=100
    ```
    """
    scraper = TwitterScrapper()
    
    results = await scraper.search(
        query=q,
        include_filters=include_filters,
        exclude_filters=exclude_filters,
        since=str(since) if since else None,
        until=str(until) if until else None,
        max_tweets=max_tweets
    )
    
    return {
        "query": q,
        "filters": {
            "include": include_filters,
            "exclude": exclude_filters
        },
        "date_range": {
            "since": since,
            "until": until
        },
        "max_tweets": max_tweets,
        "results": results
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

